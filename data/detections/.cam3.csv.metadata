{"timestamp": 1720797312.261338, "stored_source_code": "\"\"\" Ce script \u00e0 pour but de r\u00e9aliser la d\u00e9tection automatique \u00e0 partir de notre r\u00e9seau CNN.\n    \n    Dans un premier temps, il r\u00e9cup\u00e8re les donn\u00e9es issues du fichiers de synchronisation native de dartfish. \n    On s'occure dans un premier temps de r\u00e9cup\u00e9rer la valeur de SynchronizationHandicap. Cette valeur va nous servir \u00e0 synchroniser nos detections en utilsiant un timecode externe synchronis\u00e9\n    et en nous donnant une valeur de frame de d\u00e9part poiur chaque cam\u00e9ra.\n    \n    Dans un second temps, on part de la source vid\u00e9o (ici mp4) pour extraire un fichier csv de detections contenant : ['frame', 'id', 'conf', 'x1', 'y1', 'x2', 'y2']\n    - La frame de la capture\n    - L'identifiant de l'objet d\u00e9tect\u00e9 via l'utilisation d'un tracker issues de BotSort\n    - La confiance de la d\u00e9tection\n    - Les coordonn\u00e9es des bounding boxes x1, y1, x2, y2\"\"\"\nupstream = None\nvideo_path = None\noutput_path = None\ndartclip_path = None\nmodel_path = None\ntracker_path = None\nimport re\nimport math\nimport os\nimport cv2 \nfrom ultralytics import YOLO\nimport numpy as np\nimport pandas as pd\ndef extract_synchronization_handicap(file_path):\n    \"\"\" Permet de sortir le retard associ\u00e9 au Timecode de la cam\u00e9ra.\n    \n        filepath: lien du fichier dartclip \n        \"\"\"\n    # ouvrir le fichier\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n\n    # Utiliser une expression r\u00e9guli\u00e8re pour extraire la valeur de SynchronizationHandicap\n    match = re.search(r'SynchronizationHandicap=\"(\\d+)\"', content)\n    \n    if match:\n        synchronization_handicap = match.group(1)\n        return synchronization_handicap\n    else:\n        return None\n# Application \nsynchronization_handicap = extract_synchronization_handicap(dartclip_path)\n# Transformer la valeur issue de la formule en entier \nsynchronization_handicap = int(synchronization_handicap)\n# Expressio que l'on a d\u00e9termin\u00e9 pour obtenir le retard en frame (30 fps ici)\nframe_begin = synchronization_handicap/330000\n#Arrondir \u00e0 la frame inf\nframe_begin = math.floor(frame_begin)\nprint(frame_begin)\nclass VideoProcessor:\n    \n    \"\"\" Initialise un processeur de vid\u00e9o pour d\u00e9tecter des objets avec YOLO.\n    model : Mod\u00e8le YOLO \u00e0 utiliser pour la d\u00e9tection d'objets.\n    video_path : Chemin de la vid\u00e9o \u00e0 traiter.\n    output_video_path : Chemin de la vid\u00e9o de sortie annot\u00e9e avec les bounding boxes.\n    df : Dataframe contenant les donn\u00e9es de la vid\u00e9o.\n    \"\"\"\n    # Initialisation des diff\u00e9rents param\u00e8tres \n    def __init__(self, model_path, video_path):\n        self.model = self.load_model(model_path) # Charger le mod\u00e8le CNN\n        self.cap = self.load_video(video_path) # Charger la vid\u00e9o \n        self.output_video_path = video_path.replace('.mp4', '_annotated.mp4') # Pr\u00e9parer le fichier de sortie\n        self.data = pd.DataFrame(columns=['frame', 'id', 'conf', 'x1', 'y1', 'x2', 'y2']) # Pr\u00e9parer le DataFrame en initalisant les colonnes\n\n    # charger le mod\u00e8le YOLO\n    def load_model(self, path_model): \n        \"\"\" Fonction qui permet de charger le mod\u00e8le YOLO\"\"\"  \n        model = YOLO(path_model)\n        return model\n\n    # charger la vid\u00e9o\n    def load_video(self, video_path):\n        \"\"\" Fonction qui permet de charger la vid\u00e9o\"\"\"\n        cap = cv2.VideoCapture(video_path) # Charger la vid\u00e9o \u00e0 partir d'opencv (https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html)\n        if not cap.isOpened():\n            print(\"Erreur lors du chargement de la vid\u00e9o.\")\n            return None\n        else:\n            print(\"Vid\u00e9o charg\u00e9e avec succ\u00e8s.\")\n            return cap\n\n    # charger le tracker\n    def detect_objects_yolo(self, image):\n        \"\"\" Charger le tracker pour suivre les athl\u00e8tes au cours du temps. Deux possibilit\u00e9s : \n        - Soit on utilise le tracker de BotSort (data/tracker/botsort.yaml)\n        - soit on utilise le tracker de bytetrack (data/tracker/bytetrack.yaml)\n        \"\"\"\n        results = self.model.track(image, persist=True, save_dir=\"output\", tracker= tracker_path)\n        return results\n    \n\n\n    # traiter la vid\u00e9o\n    def process_video(self):\n        \"\"\" D\u00e9finir le processus de traitement de la vid\u00e9o\n            1) R\u00e9cup\u00e9rer les informations issues de la vid\u00e9os originale (fps, frame_width, frame_height)\n            2) D\u00e9finir le codec et cr\u00e9er l'objet VideoWriter\n            3) D\u00e9finir la frame de d\u00e9part (\u00e0 partir de la fonction extract_synchronization_handicap)\n            4) Boucle de traitement de la vid\u00e9o\n            5) \u00c9crire le frame annot\u00e9 dans la vid\u00e9o de sortie et sauvegarder les donn\u00e9es dans un fichier csv\n        \"\"\"\n        frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # Obtenir les propri\u00e9t\u00e9s de la vid\u00e9o originale\n        frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # Obtenir les propri\u00e9t\u00e9s de la vid\u00e9o originale\n        fps = int(self.cap.get(cv2.CAP_PROP_FPS)) # Obtenir les propri\u00e9t\u00e9s de la vid\u00e9o originale\n\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v') # D\u00e9finir le codec et cr\u00e9er l'objet VideoWriter\n        out = cv2.VideoWriter(self.output_video_path, fourcc, fps, (frame_width, frame_height)) # D\u00e9finir le codec et cr\u00e9er l'objet VideoWriter (https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html)\n        \n        # D\u00e9finir la frame de d\u00e9part\n        frame_count = frame_begin\n\n        # Boucle de traitement de la vid\u00e9o\n        while self.cap.isOpened():\n            ret, frame = self.cap.read() # valider l'ouverture de la vid\u00e9o et r\u00e9cup\u00e9rer le frame\n            if not ret:\n                break\n            \n            # Appliquer le mod\u00e8le YOLO pour d\u00e9tecter les objets dans le frame\n            results = self.detect_objects_yolo(frame) # D\u00e9tecter les objets dans le frame\n            # annoter la frame\n            annotated_frame = results[0].plot(conf = False, line_width = 1, font_size = 10)\n            \n            # R\u00e9cup\u00e9rer les informations des objets d\u00e9tect\u00e9s\n            if results[0].boxes is not None and len(results[0].boxes) > 0: # V\u00e9rifier si des objets ont \u00e9t\u00e9 d\u00e9tect\u00e9s\n                confs = results[0].boxes.conf.cpu().numpy() if results[0].boxes.conf is not None else np.array([]) # R\u00e9cup\u00e9rer les confiances des objets d\u00e9tect\u00e9s\n                x1s = results[0].boxes.xyxy[:, 0].cpu().numpy() if results[0].boxes.xyxy is not None else np.array([]) # R\u00e9cup\u00e9rer les coordonn\u00e9es x1 des objets d\u00e9tect\u00e9s\n                y1s = results[0].boxes.xyxy[:, 1].cpu().numpy() if results[0].boxes.xyxy is not None else np.array([]) # R\u00e9cup\u00e9rer les coordonn\u00e9es y1 des objets d\u00e9tect\u00e9s\n                x2s = results[0].boxes.xyxy[:, 2].cpu().numpy() if results[0].boxes.xyxy is not None else np.array([]) # R\u00e9cup\u00e9rer les coordonn\u00e9es x2 des objets d\u00e9tect\u00e9s\n                y2s = results[0].boxes.xyxy[:, 3].cpu().numpy() if results[0].boxes.xyxy is not None else np.array([]) # R\u00e9cup\u00e9rer les coordonn\u00e9es y2 des objets d\u00e9tect\u00e9s\n                classes = results[0].boxes.cls.cpu().numpy() if results[0].boxes.cls is not None else np.array([])  # R\u00e9cup\u00e9rer les classes des objets d\u00e9tect\u00e9s (ici dans notre cas seulement BMX)\n                ids = results[0].boxes.id.cpu().numpy() if results[0].boxes.id is not None else np.array([]) # R\u00e9cup\u00e9rer les identifiants des objets d\u00e9tect\u00e9s\n                \n                # Ajouter les informations des objets d\u00e9tect\u00e9s dans le DataFrame sous la forme d'une boucle\n                for i in range(len(confs)):\n                    # V\u00e9rifier que tous les attributs ont des valeurs \u00e0 l'indice i\n                    if i < len(ids) and i < len(confs) and i < len(x1s) and i < len(y1s) and i < len(x2s) and i < len(y2s) and i < len(classes):\n                        # Ajouter les informations des objets d\u00e9tect\u00e9s dans le DataFrame\n                        self.data.loc[len(self.data)] = {\n                            'frame': frame_count,\n                            'id': ids[i],\n                            'conf': confs[i],\n                            'x1': x1s[i],\n                            'y1': y1s[i],\n                            'x2': x2s[i],\n                            'y2': y2s[i],\n                            'class': classes[i]\n                        }\n\n            # \u00c9crire le frame annot\u00e9 dans la vid\u00e9o de sortie\n            out.write(annotated_frame)\n            \n            # Afficher l'image avec les bounding boxes\n            cv2.imshow('YOLO Object Detection', annotated_frame)\n            \n            # Temps de latence et affichage de la vid\u00e9o + arret de la vid\u00e9o si pression sur le q\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n            \n            # Ajouter une frame au compteur\n            frame_count += 1\n         \n        # Relacher la capture et fermer les fen\u00eatres\n        self.cap.release()\n        # Fermer la vid\u00e9o de sortie\n        out.release()\n        # D\u00e9truire toutes les fen\u00eatres\n        cv2.destroyAllWindows()\n        \n        # Exporter les donn\u00e9es du df dans un csv \n        self.data.to_csv(output_path, index=False)\n\ndef run_yolo(output_path):\n    \"\"\"  Si le fichier de sortie n'existe pas, on lance le traitement de la vid\u00e9o avec le mod\u00e8le YOLO.\n        Sinon, on use la d\u00e9tection d\u00e9j\u00e0 existante\"\"\"\n    if not os.path.exists(output_path):        \n        processor = VideoProcessor(model_path, video_path) # Initialiser le processeur de vid\u00e9o \u00e0 partir de la class initialis\u00e9e\n        processor.process_video() # Lancer le traitement de la vid\u00e9o\n        \n    else:\n        print(f\"Detections already exist at {output_path}, skipping CNN.\")\n        \nrun_yolo(output_path)", "params": {"video_path": "data/videos/cam3.mp4", "dartclip_path": "data/dartclip/cam3.dartclip", "output_path": "data/detections/cam3.csv", "model_path": "data/model/76_6_l.pt", "tracker_path": "data/tracker/botsort.yaml"}}